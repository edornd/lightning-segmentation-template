model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: ${model.monitored_metric}  # name of the logged metric which determines when model is improving
    save_top_k: 5                       # save k best models (determined by above metric)
    save_last: True                     # additionaly always save model from last epoch
    mode: ${model.monitored_mode}        # can be "max" or "min"
    verbose: True
    dirpath: checkpoints/
    filename: "{epoch:02d}-{step}"

early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: ${model.monitored_metric}  # name of the logged metric which determines when model is improving
    patience: 20                        # how many epochs of not improving until training stops
    mode: ${model.monitored_mode}        # can be "max" or "min"
    min_delta: 0.001                    # minimum change in the monitored metric needed to qualify as an improvement

wandb_watch_model:
    _target_: src.callbacks.wandb.WatchModelWithWandb
    log: "all"
    log_freq: 100

wandb_log_masks:
    _target_: src.callbacks.wandb.LogPredictionMasks
    ignore_index: ${datamodule.ignore_index}
    class_names: ${datamodule.class_names}
